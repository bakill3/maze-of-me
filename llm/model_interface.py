"""
Phi-3 Mini LLaMA wrapper â€“ only NPC speech.
"""

from __future__ import annotations
import os
import json
from pathlib import Path
from llama_cpp import Llama
from config import Config
import threading
from functools import lru_cache
import hashlib

MODEL_NAME = "Phi-3-mini-4k-instruct-q4.gguf"
MODEL_PATH = Config.MODELS_DIR / MODEL_NAME

print(f"\n[INFO] Loading model from: {MODEL_PATH}")
print(f"[INFO] Model file exists: {MODEL_PATH.exists()}")

if not MODEL_PATH.exists():
    raise FileNotFoundError(
        f"\n[ERROR] Model not found: {MODEL_PATH}\n"
        f"Please run 'download.bat' in the 'models' folder before starting the game.\n"
    )

# Use a safe, low context window and thread count for Windows stability
_llm = Llama(
    model_path=str(MODEL_PATH),
    n_ctx=768,  # Lower context window for even faster response
    n_threads=4, # More threads for more speed (tune for your CPU)
    verbose=False,
)

# Read given name for prompt stopping
try:
    GIVEN = json.loads(Config.PROFILE_PATH.read_text("utf-8")).get("full_name", "").split()[0]
except Exception:
    GIVEN = ""

# STOP = ["\n", "Assistant:", f"{GIVEN}:", "<END>"]
STOP = ["Assistant:", f"{GIVEN}:", "<END>"]

# Simple in-memory cache for LLM responses (prompt -> reply)
_llm_cache = {}

LLM_LOCK = threading.Lock()

FALLBACK_NPC = "The figure seems lost in thought and does not respond right now."

def _run(prompt: str, max_tokens: int, temperature: float) -> str:
    # Aggressive cache: use hash of prompt for similar situations
    prompt_hash = hashlib.sha256(prompt.encode("utf-8")).hexdigest()
    cache_key = (prompt_hash, max_tokens, temperature)
    if cache_key in _llm_cache:
        return _llm_cache[cache_key]
    try:
        with LLM_LOCK:
            res = _llm(prompt=prompt, max_tokens=max_tokens, temperature=temperature, stop=STOP)
        text = res["choices"][0]["text"].strip()
        if text:
            _llm_cache[cache_key] = text
        return text
    except Exception as e:
        print(f"[ERROR] Llama model inference failed: {e}")
        return ""

def query_npc(prompt: str, max_tokens: int = 100) -> str:
    # Try up to 2 times, fallback if still empty
    for _ in range(2):
        reply = _run(prompt, max_tokens=max_tokens, temperature=0.8)
        if reply:
            return reply
    return FALLBACK_NPC

def streaming_query_npc(prompt: str, max_tokens: int = 100, temperature: float = 0.8):
    """Yield tokens as they are generated by the Llama.cpp model (if supported)."""
    try:
        with LLM_LOCK:
            got_chunk = False
            for chunk in _llm(
                prompt=prompt,
                max_tokens=max_tokens,
                temperature=temperature,
                stop=STOP,
                stream=True
            ):
                token = chunk["choices"][0]["text"]
                if token:
                    got_chunk = True
                    yield token
            if not got_chunk:
                yield "[NO CHUNKS]"
    except Exception as e:
        print(f"[ERROR] Streaming Llama model inference failed: {e}")
        yield "[AI error]"
